{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask provides multi-core and distributed parallel execution on larger-than-memory datasets.\n",
    "\n",
    "We can think of dask at a high and a low level\n",
    "\n",
    "*  **High level collections:**  dask provides high-level Array, Bag, and DataFrame\n",
    "   collections that mimic NumPy, lists, and Pandas but can operate in parallel on\n",
    "   datasets that don't fit into memory.  Dask's high-level collections are\n",
    "   alternatives to NumPy and Pandas for large datasets. Dask functions are \n",
    "   alternative to Spark and MapReduce.\n",
    "   \n",
    "*  **Low Level schedulers:** dask provides dynamic task schedulers that\n",
    "   execute task graphs in parallel.  These execution engines power the\n",
    "   high-level collections mentioned above but can also power custom,\n",
    "   user-defined workloads.  These schedulers are low-latency (around 1ms) and\n",
    "   work hard to run computations in a small memory footprint.  Dask's\n",
    "   schedulers are an alternative to direct use of `threading` or\n",
    "   `multiprocessing` libraries in complex cases or other task scheduling\n",
    "   systems like `Luigi` or `IPython parallel`.\n",
    "  \n",
    "<img src=\"collections-schedulers.png\" align=\"center\" width=\"60%\">\n",
    "\n",
    "We are planning to use only scheduler, but in future we can incorporate MapReduce functionality of dask in mne-python. \n",
    "\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "Dask need the following core libraries\n",
    "\n",
    "    conda install numpy pandas h5py Pillow matplotlib scipy toolz pytables fastparquet\n",
    "\n",
    "Install dask distributed\n",
    "\n",
    "    conda install dask distributed\n",
    "\n",
    "The following is useful for task graph visualization\n",
    "\n",
    "    conda install graphviz\n",
    "\n",
    "We also need glob for reading directory contents\n",
    "\n",
    "     conda install glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mne\n",
    "import glob\n",
    "import timeit\n",
    "import numpy as np\n",
    "from dask import delayed\n",
    "from dask.distributed import LocalCluster, Client\n",
    "from mne.time_frequency import psd_multitaper\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reduce Verbosity\n",
    "mne.set_log_level('WARNING')\n",
    "# Numbers of subjects to be processed in parallel\n",
    "nsubjects_parallel = range(1,101,10)\n",
    "# For sanity only first 100 subject out of 650"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reading Directory Structure\n",
    "fif_files_path = '/autofs/cluster/fusion/Sheraz/data/camcan/camcan47' \\\n",
    "'/cc700/meg/pipeline/release004/data_nomovecomp' \\\n",
    "'/aamod_meg_maxfilt_00001/*/rest/transdef_mf2pt2_rest_raw.fif'\n",
    "\n",
    "files = glob.glob(fif_files_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_psd(fif_file):\n",
    "    raw = mne.io.read_raw_fif(fif_file, preload=True)\n",
    "    raw.crop(50, 150)\n",
    "    picks = mne.pick_types(raw.info, meg='mag', eeg=False, \n",
    "                           eog=False, stim=False)\n",
    "    psd, _ = psd_multitaper(raw, fmin=2, fmax=55, picks=picks, normalization=\"full\")\n",
    "    return np.log10(psd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sequential code on local Workstation 32 Nodes\n",
    "\n",
    "time_elpased_sequential = []\n",
    "\n",
    "for nsubjets in nsubjects_parallel:\n",
    "    psds = []\n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    for file in files[0:nsubjets]:\n",
    "        psd = compute_psd(file)\n",
    "        psds.append(psd)\n",
    "    mean_psd = np.mean(psds)\n",
    "    time_elpased_sequential.append(timeit.default_timer() - start_time)\n",
    "    \n",
    "print(time_elpased_sequential)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parallel Code  on local Scheduler 32 Nodes\n",
    "\n",
    "cluster = LocalCluster()\n",
    "client  = Client(cluster)\n",
    "\n",
    "time_elpased_lparallel = []\n",
    "\n",
    "for nsubjets in nsubjects_parallel:\n",
    "    psds = []\n",
    "    \n",
    "    for file in files[0:nsubjets]:\n",
    "        psd= delayed(compute_psd)(file)\n",
    "        psds.append(psd)\n",
    "        \n",
    "    mean_psd_delayed = delayed(np.mean)(psds, axis=1)\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    mean_psd = client.compute(mean_psd_delayed)\n",
    "    mean_psd = mean_psd.result()\n",
    "    time_elpased_lparallel.append(timeit.default_timer() - start_time)\n",
    "    \n",
    "print(time_elpased_lparallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_psd_delayed.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parallel Code  on Martinos Scheduler\n",
    "\n",
    "client = Client('172.21.16.27:8786')\n",
    "\n",
    "time_elpased_mparallel = []\n",
    "\n",
    "for nsubjets in nsubjects_parallel:\n",
    "    psds = []\n",
    "    \n",
    "    for file in files[0:nsubjets]:\n",
    "        psd= delayed(compute_psd)(file)\n",
    "        psds.append(psd)\n",
    "        \n",
    "    mean_psd_delayed = delayed(np.mean)(psds, axis=1)\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    mean_psd = client.compute(mean_psd_delayed)\n",
    "    mean_psd = mean_psd.result()\n",
    "    time_elpased_mparallel.append(timeit.default_timer() - start_time)\n",
    "    \n",
    "print(time_elpased_mparallel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Get AWS setup\n",
    "\n",
    "1. From https://boto3.readthedocs.io/en/latest/guide/quickstart.html (Boto is the Amazon Web Services (AWS) SDK for Python)\n",
    "\n",
    "        sudo apt-get install awscli\n",
    "        pip install boto3\n",
    "2. Visit AWS -> IAM -> Add user -> Security Credentials -> Create Access Key\n",
    "3. Run `aws configure` and enter the ID, code, region (us-east-1), outputformat (blank - leave as JSON)\n",
    "4. Test with:\n",
    "\n",
    "        import boto3\n",
    "        s3 = boto3.resource('s3')\n",
    "        for b in s3.buckets.all():\n",
    "            print(b.name)\n",
    "        \n",
    "5. From http://distributed.readthedocs.io/en/latest/ec2.html,\n",
    "\n",
    "        pip install dask-ec2\n",
    "\n",
    "6. Visit AWS->EC2->Key pairs->Create key pair. I called mine \"mitkey\". Save the keyfile in .ssh, chmod 600.\n",
    "7. Get the AMI we want to use (e.g. ubuntu 14.04). Check https://cloud-images.ubuntu.com/locator/ec2/ and search for e.g. `14.04 LTS us-east-1 hvm ebs`.\n",
    "(Also found using ubuntu 16.04 had a SSL wrong version number error (see https://github.com/dask/dask-ec2/issues/38 )\n",
    "\n",
    "## Running DASK\n",
    "\n",
    "1. Run `dask-ec2 up --keyname YOUR-AWS-KEY --keypair ~/.ssh/YOUR-AWS-SSH-KEY.pem`. I found I had to also specify the ami and tags as the first two have wrong defaults and the tool seems to fail if tags isn't set either.\n",
    "\n",
    "E.g.\n",
    "\n",
    "Low Workload (2 instances x with 2 cCPUs = $0.092800*2/hour):\n",
    "\n",
    "dask-ec2 up --keyname mitkey --keypair ~/keys/mit.pem  --ami ami-cee00cb4 --tags research:dp --type t2.large --count 2 --volume-size 30\n",
    "      \n",
    "or High Workload (100 instances x with 2 cCPUs = $0.092800*100/hour):\n",
    "\n",
    "dask-ec2 up --keyname mitkey --keypair ~/keys/mit.pem  --ami ami-cee00cb4 --tags research:dp --type t2.large --count 100 --volume-size 30\n",
    "        \n",
    "Eventually after a long time, this will finish with:\n",
    "\n",
    "        Dask.Distributed Installation succeeded\n",
    "\n",
    "        Addresses\n",
    "        ---------\n",
    "        Web Interface:    http://54.246.253.159:8787/status\n",
    "        TCP Interface:           54.246.253.159:8786\n",
    "\n",
    "        To connect from the cluster\n",
    "        ---------------------------\n",
    "\n",
    "        dask-ec2 ssh  # ssh into head node\n",
    "        ipython  # start ipython shell\n",
    "\n",
    "        from dask.distributed import Client, progress\n",
    "        c = Client('127.0.0.1:8786')  # Connect to scheduler running on the head node\n",
    "\n",
    "        To connect locally\n",
    "        ------------------\n",
    "\n",
    "        Note: this requires you to have identical environments on your local machine and cluster.\n",
    "\n",
    "        ipython  # start ipython shell\n",
    "\n",
    "        from dask.distributed import Client, progress\n",
    "        e = Client('54.246.253.159:8786')  # Connect to scheduler running on the head node\n",
    "\n",
    "        To destroy\n",
    "        ----------\n",
    "\n",
    "        dask-ec2 destroy\n",
    "        \n",
    "Make sure to run dask-ec2 destroy, and check it on AWS dashboard.\n",
    "\n",
    "I already created this ami (ami-cee00cb4), which contains some additional packages\n",
    "\n",
    "s3fs: to access files from s3\n",
    "boto3: for aws commands\n",
    "\n",
    "This ami as of now, do not support GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
